{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "def query(query_text: str, limit: int = 5) -> list[str]:\n",
    "    \"\"\"\n",
    "    Query the Qdrant vector database with a text query and return matching results.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): The text query to search for\n",
    "        limit (int): Maximum number of results to return (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "        list[str]: List of matching text results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Generate embedding for the query text\n",
    "        query_embedding = embedding_model.embed_query(query_text)\n",
    "        \n",
    "        # Search in Qdrant\n",
    "        search_results = qdrant_client.search(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            query_vector=query_embedding,\n",
    "            limit=limit\n",
    "        )\n",
    "        \n",
    "        # Extract text content from results\n",
    "        results = []\n",
    "        for result in search_results:\n",
    "            # Assuming the text content is stored in payload under 'text' key\n",
    "            # Adjust the key name based on your actual data structure\n",
    "            if 'text' in result.payload:\n",
    "                results.append(result.payload['text'])\n",
    "            elif 'content' in result.payload:\n",
    "                results.append(result.payload['content'])\n",
    "            else:\n",
    "                # If no text field found, convert payload to string\n",
    "                results.append(str(result.payload))\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during query: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_AsyncGeneratorContextManager' object has no attribute 'get_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m rag_client = client.session(\u001b[33m\"\u001b[39m\u001b[33mRAGService\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m document_client = client.session(\u001b[33m\"\u001b[39m\u001b[33mDocumentService\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m rag_tools = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43mrag_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tools\u001b[49m()\n\u001b[32m     42\u001b[39m document_tools = \u001b[38;5;28;01mawait\u001b[39;00m document_client.get_tools()\n\u001b[32m     44\u001b[39m prompt = \u001b[33m\"\u001b[39m\u001b[33mYou are a RAG agent, please query the RAGService for the information if the user asks\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: '_AsyncGeneratorContextManager' object has no attribute 'get_tools'"
     ]
    }
   ],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "import time\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "AZURE_OPENAI_API_KEY=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_MODEL_NAME=os.getenv(\"AZURE_OPENAI_MODEL_NAME\")\n",
    "AZURE_OPENAI_MODEL_API_VERSION=os.getenv(\"AZURE_OPENAI_MODEL_API_VERSION\")\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    model=AZURE_OPENAI_MODEL_NAME,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_MODEL_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    temperature=0,\n",
    "    max_tokens=5000\n",
    ")\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"RAGService\": {\n",
    "            \"url\": \"http://localhost:8002/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "        },\n",
    "        \"DocumentService\": {\n",
    "            \"url\": \"http://localhost:8001/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()\n",
    "prompt = \"You are a RAG agent, please query the RAGService for the information if the user asks\"\n",
    "agent = create_react_agent(model, tools, prompt=prompt)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "answer = await astream_graph(\n",
    "    agent, {\"messages\": \"What's MCP? Answer with out query\"}\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 12.563843965530396 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing file: mcp.md\n",
      "Collection 'mcp' does not exist. Creating it.\n",
      "Extracting text from mcp.md...\n",
      "Text extracted (first 200 chars): # Meeting Minutes\n",
      "\n",
      "## June 19, 2025\n",
      "\n",
      "### Attendees\n",
      "- Alice\n",
      "- Bob\n",
      "- Charlie\n",
      "\n",
      "### Discussion Points\n",
      "1.  **Project Alpha**: Reviewed progress. On track for phase 1 completion.\n",
      "2.  **Budget Review**: Disc...\n",
      "Processing and adding chunks to Qdrant using 'markdown_header' method...\n",
      "Successfully upserted 3 chunks to Qdrant for document ID: mcp.md\n",
      "Document processing complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_mcp_adapters.client import MCPClient\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Azure OpenAI Configuration\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_MODEL_NAME = os.getenv(\"AZURE_OPENAI_MODEL_NAME\")\n",
    "AZURE_OPENAI_MODEL_API_VERSION = os.getenv(\"AZURE_OPENAI_MODEL_API_VERSION\")\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    model=AZURE_OPENAI_MODEL_NAME,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_MODEL_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    temperature=0,\n",
    "    max_tokens=5000\n",
    ")\n",
    "\n",
    "async def setup_agents():\n",
    "    \"\"\"Setup individual MCP clients and agents\"\"\"\n",
    "    \n",
    "    # RAG Service MCP Client\n",
    "    rag_client = MCPClient(\n",
    "        url=\"http://localhost:8002/sse\",\n",
    "        transport=\"sse\"\n",
    "    )\n",
    "    \n",
    "    # Document Service MCP Client  \n",
    "    document_client = MCPClient(\n",
    "        url=\"http://localhost:8001/sse\",\n",
    "        transport=\"sse\"\n",
    "    )\n",
    "    \n",
    "    # Get tools from each service\n",
    "    rag_tools = await rag_client.get_tools()\n",
    "    document_tools = await document_client.get_tools()\n",
    "    \n",
    "    # Create RAG Agent\n",
    "    rag_agent = create_react_agent(\n",
    "        model=model,\n",
    "        tools=rag_tools,\n",
    "        name=\"rag_expert\",\n",
    "        prompt=(\n",
    "            \"You are a RAG (Retrieval-Augmented Generation) expert. \"\n",
    "            \"Use vector search to find relevant information from the knowledge base. \"\n",
    "            \"Provide accurate, contextual answers based on retrieved documents. \"\n",
    "            \"Always cite your sources when possible.\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Create Document Agent\n",
    "    document_agent = create_react_agent(\n",
    "        model=model,\n",
    "        tools=document_tools,\n",
    "        name=\"document_expert\", \n",
    "        prompt=(\n",
    "            \"You are a document processing expert. \"\n",
    "            \"Handle document operations like reading, parsing, and extracting information. \"\n",
    "            \"Process various file formats and provide structured summaries. \"\n",
    "            \"Focus on accuracy and completeness in document analysis.\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return rag_agent, document_agent\n",
    "\n",
    "async def create_supervisor_workflow():\n",
    "    \"\"\"Create the supervisor workflow with specialized agents\"\"\"\n",
    "    \n",
    "    # Setup agents\n",
    "    rag_agent, document_agent = await setup_agents()\n",
    "    \n",
    "    # Create supervisor workflow\n",
    "    workflow = create_supervisor(\n",
    "        agents=[rag_agent, document_agent],\n",
    "        model=model,\n",
    "        prompt=(\n",
    "            \"You are a team supervisor managing specialized AI agents. \"\n",
    "            \"Route tasks based on their nature:\\n\"\n",
    "            \"- For knowledge retrieval, semantic search, or answering questions from existing data: use rag_expert\\n\"\n",
    "            \"- For document processing, file analysis, or content extraction: use document_expert\\n\"\n",
    "            \"Always choose the most appropriate agent for the task.\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return workflow\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    # Create supervisor workflow\n",
    "    workflow = await create_supervisor_workflow()\n",
    "    \n",
    "    # Compile the workflow\n",
    "    app = workflow.compile()\n",
    "    \n",
    "    # Example usage\n",
    "    result = await app.ainvoke({\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": \"What's MCP? Please query the RAGService for information.\"\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(\"=== Supervisor Response ===\")\n",
    "    for message in result[\"messages\"]:\n",
    "        print(f\"{message['role']}: {message['content']}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Alternative synchronous wrapper for easier usage\n",
    "def run_supervisor_query(query: str):\n",
    "    \"\"\"Synchronous wrapper for running supervisor queries\"\"\"\n",
    "    \n",
    "    async def _run():\n",
    "        workflow = await create_supervisor_workflow()\n",
    "        app = workflow.compile()\n",
    "        \n",
    "        result = await app.ainvoke({\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
    "        })\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    return asyncio.run(_run())\n",
    "\n",
    "# Example usage functions\n",
    "async def example_rag_query():\n",
    "    \"\"\"Example RAG query\"\"\"\n",
    "    workflow = await create_supervisor_workflow()\n",
    "    app = workflow.compile()\n",
    "    \n",
    "    result = await app.ainvoke({\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Search for information about machine learning algorithms in the knowledge base.\"\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    return result\n",
    "\n",
    "async def example_document_query():\n",
    "    \"\"\"Example document processing query\"\"\"\n",
    "    workflow = await create_supervisor_workflow()\n",
    "    app = workflow.compile()\n",
    "    \n",
    "    result = await app.ainvoke({\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": \"Please analyze and summarize the contents of the uploaded PDF document.\"\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the main example\n",
    "    asyncio.run(main())\n",
    "    \n",
    "    # Or use the synchronous wrapper\n",
    "    # result = run_supervisor_query(\"What's MCP? Please search the knowledge base.\")\n",
    "    # print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Supervisor Response ===\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'HumanMessage' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 189\u001b[39m\n\u001b[32m    183\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    188\u001b[39m     \u001b[38;5;66;03m# This only runs when executed as a script, not in Jupyter\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 144\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Supervisor Response ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m result[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmessage\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage[\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    145\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'HumanMessage' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from mcp import ClientSession\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Azure OpenAI Configuration\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_MODEL_NAME = os.getenv(\"AZURE_OPENAI_MODEL_NAME\")\n",
    "AZURE_OPENAI_MODEL_API_VERSION = os.getenv(\"AZURE_OPENAI_MODEL_API_VERSION\")\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    model=AZURE_OPENAI_MODEL_NAME,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_MODEL_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    temperature=0,\n",
    "    max_tokens=5000\n",
    ")\n",
    "\n",
    "async def setup_rag_agent():\n",
    "    \"\"\"Setup RAG agent with single MCP client\"\"\"\n",
    "    \n",
    "    # Connect to RAG Service using streamablehttp_client\n",
    "    async with streamablehttp_client(\"http://localhost:8002/mcp/\") as (read, write, _):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            # Initialize the connection\n",
    "            await session.initialize()\n",
    "            \n",
    "            # Get tools from RAG service\n",
    "            rag_tools = await load_mcp_tools(session)\n",
    "            \n",
    "            # Create RAG Agent\n",
    "            rag_agent = create_react_agent(\n",
    "                model=model,\n",
    "                tools=rag_tools,\n",
    "                name=\"rag_expert\",\n",
    "                prompt=(\n",
    "                    \"You are a RAG (Retrieval-Augmented Generation) expert. \"\n",
    "                    \"Use vector search to find relevant information from the knowledge base. \"\n",
    "                    \"Provide accurate, contextual answers based on retrieved documents. \"\n",
    "                    \"Always cite your sources when possible.\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            return rag_agent, session  # Return session to keep connection alive\n",
    "\n",
    "async def create_supervisor_workflow():\n",
    "    \"\"\"Create the supervisor workflow with RAG agent only\"\"\"\n",
    "    \n",
    "    # Setup RAG agent\n",
    "    rag_agent, session = await setup_rag_agent()\n",
    "    \n",
    "    # Create supervisor workflow\n",
    "    workflow = create_supervisor(\n",
    "        agents=[rag_agent],\n",
    "        model=model,\n",
    "        prompt=(\n",
    "            \"You are a team supervisor managing a RAG expert. \"\n",
    "            \"Route knowledge retrieval, semantic search, or question answering tasks to rag_expert. \"\n",
    "            \"Always use the rag_expert for information retrieval from the knowledge base.\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return workflow, session\n",
    "\n",
    "# Alternative approach using MultiServerMCPClient for single server\n",
    "async def setup_rag_agent_alternative():\n",
    "    \"\"\"Alternative setup using MultiServerMCPClient with single server\"\"\"\n",
    "    from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "    \n",
    "    # Single server configuration\n",
    "    client = MultiServerMCPClient({\n",
    "        \"rag_service\": {\n",
    "            \"url\": \"http://localhost:8002/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    # Get tools\n",
    "    rag_tools = await client.get_tools()\n",
    "    \n",
    "    # Create RAG Agent\n",
    "    rag_agent = create_react_agent(\n",
    "        model=model,\n",
    "        tools=rag_tools,\n",
    "        name=\"rag_expert\",\n",
    "        prompt=(\n",
    "            \"You are a RAG expert. \"\n",
    "            \"Search the knowledge base to answer questions. \"\n",
    "            \"Provide accurate answers with source citations.\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return rag_agent\n",
    "\n",
    "async def create_supervisor_workflow_alternative():\n",
    "    \"\"\"Alternative supervisor workflow\"\"\"\n",
    "    \n",
    "    # Setup RAG agent\n",
    "    rag_agent = await setup_rag_agent_alternative()\n",
    "    \n",
    "    # Create supervisor workflow\n",
    "    workflow = create_supervisor(\n",
    "        agents=[rag_agent],\n",
    "        model=model,\n",
    "        prompt=(\n",
    "            \"You manage a RAG expert. \"\n",
    "            \"For any knowledge questions or information retrieval: use rag_expert.\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return workflow\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main execution function - using alternative approach\"\"\"\n",
    "    \n",
    "    # Create supervisor workflow (using simpler MultiServerMCPClient approach)\n",
    "    workflow = await create_supervisor_workflow_alternative()\n",
    "    \n",
    "    # Compile the workflow\n",
    "    app = workflow.compile()\n",
    "    \n",
    "    # Example usage\n",
    "    result = await app.ainvoke({\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": \"What's MCP? Please query the RAGService for information.\"\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(\"=== Supervisor Response ===\")\n",
    "    for message in result[\"messages\"]:\n",
    "        print(f\"{message['role']}: {message['content']}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Synchronous wrapper\n",
    "def run_supervisor_query(query: str):\n",
    "    \"\"\"Synchronous wrapper for running supervisor queries\"\"\"\n",
    "    \n",
    "    async def _run():\n",
    "        workflow = await create_supervisor_workflow_alternative()\n",
    "        app = workflow.compile()\n",
    "        \n",
    "        result = await app.ainvoke({\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
    "        })\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    return asyncio.run(_run())\n",
    "\n",
    "# Test functions\n",
    "async def test_rag_query():\n",
    "    \"\"\"Test RAG query\"\"\"\n",
    "    workflow = await create_supervisor_workflow_alternative()\n",
    "    app = workflow.compile()\n",
    "    \n",
    "    result = await app.ainvoke({\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Please tell me what is Model Context Protocol?\"\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(\"=== RAG Test Response ===\")\n",
    "    for message in result[\"messages\"]:\n",
    "        print(f\"{message.role}: {message.content}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This only runs when executed as a script, not in Jupyter\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from mcp import ClientSession\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Azure OpenAI Configuration\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_MODEL_NAME = os.getenv(\"AZURE_OPENAI_MODEL_NAME\")\n",
    "AZURE_OPENAI_MODEL_API_VERSION = os.getenv(\"AZURE_OPENAI_MODEL_API_VERSION\")\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    model=AZURE_OPENAI_MODEL_NAME,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_MODEL_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    temperature=0,\n",
    "    max_tokens=5000\n",
    ")\n",
    "\n",
    "def log_time(message, start=None):\n",
    "    now = datetime.now()\n",
    "    if start:\n",
    "        duration = now - start\n",
    "        print(f\"[{now}] {message} - Duration: {duration}\")\n",
    "    else:\n",
    "        print(f\"[{now}] {message}\")\n",
    "    return now\n",
    "\n",
    "async def setup_rag_agent_alternative():\n",
    "    from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "    \n",
    "    log_time(\"Initializing MultiServerMCPClient\")\n",
    "    start = datetime.now()\n",
    "\n",
    "    client = MultiServerMCPClient({\n",
    "        \"rag_service\": {\n",
    "            \"url\": \"http://localhost:8002/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "        }\n",
    "    })\n",
    "\n",
    "    log_time(\"Fetching RAG tools from MCP\", start)\n",
    "    start = datetime.now()\n",
    "    rag_tools = await client.get_tools()\n",
    "    log_time(\"Fetched RAG tools\", start)\n",
    "\n",
    "    log_time(\"Creating RAG Agent\")\n",
    "    start = datetime.now()\n",
    "    rag_agent = create_react_agent(\n",
    "        model=model,\n",
    "        tools=rag_tools,\n",
    "        name=\"rag_expert\",\n",
    "        prompt=(\n",
    "            \"You are a RAG expert. \"\n",
    "            \"Search the knowledge base to answer questions. \"\n",
    "            \"Provide accurate answers with source citations.\"\n",
    "        )\n",
    "    )\n",
    "    log_time(\"Created RAG Agent\", start)\n",
    "    return rag_agent\n",
    "\n",
    "async def create_supervisor_workflow_alternative():\n",
    "    log_time(\"Setting up RAG Agent (Alternative)\")\n",
    "    start = datetime.now()\n",
    "    rag_agent = await setup_rag_agent_alternative()\n",
    "    log_time(\"RAG Agent setup complete\", start)\n",
    "\n",
    "    log_time(\"Creating Supervisor Workflow\")\n",
    "    start = datetime.now()\n",
    "    workflow = create_supervisor(\n",
    "        agents=[rag_agent],\n",
    "        model=model,\n",
    "        prompt=(\n",
    "            \"You manage a RAG expert. \"\n",
    "            \"For any knowledge questions or information retrieval: use rag_expert.\"\n",
    "        )\n",
    "    )\n",
    "    log_time(\"Created Supervisor Workflow\", start)\n",
    "    return workflow\n",
    "\n",
    "async def main():\n",
    "    log_time(\"Main execution started\")\n",
    "    start_total = datetime.now()\n",
    "\n",
    "    workflow = await create_supervisor_workflow_alternative()\n",
    "    \n",
    "    log_time(\"Compiling workflow\")\n",
    "    start = datetime.now()\n",
    "    app = workflow.compile()\n",
    "    log_time(\"Workflow compiled\", start)\n",
    "\n",
    "    log_time(\"Sending user query to app\")\n",
    "    start = datetime.now()\n",
    "    result = await app.ainvoke({\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": \"What's MCP? Please query the RAGService for information.\"\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "    log_time(\"Query completed\", start)\n",
    "\n",
    "    print(\"\\n=== Supervisor Response ===\")\n",
    "    for message in result[\"messages\"]:\n",
    "        print(f\"{message['role']}: {message['content']}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    log_time(\"Main execution finished\", start_total)\n",
    "\n",
    "def run_supervisor_query(query: str):\n",
    "    async def _run():\n",
    "        workflow = await create_supervisor_workflow_alternative()\n",
    "        app = workflow.compile()\n",
    "        \n",
    "        start = datetime.now()\n",
    "        result = await app.ainvoke({\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
    "        })\n",
    "        log_time(\"Synchronous query completed\", start)\n",
    "\n",
    "        return result\n",
    "\n",
    "    return asyncio.run(_run())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"web_sk.csv\"\n",
    "\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sk = df[df['Web'].str.contains('.sk')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep website that contain .sk in the url Web\n",
    "\n",
    "df_sk.to_csv(\"web_sk_categorized.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
