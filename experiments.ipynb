{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing file: mcp.md\n",
      "Collection 'mcp' does not exist. Creating it.\n",
      "Extracting text from mcp.md...\n",
      "Text extracted (first 200 chars): # Meeting Minutes\n",
      "\n",
      "## June 19, 2025\n",
      "\n",
      "### Attendees\n",
      "- Alice\n",
      "- Bob\n",
      "- Charlie\n",
      "\n",
      "### Discussion Points\n",
      "1.  **Project Alpha**: Reviewed progress. On track for phase 1 completion.\n",
      "2.  **Budget Review**: Disc...\n",
      "Processing and adding chunks to Qdrant using 'markdown_header' method...\n",
      "Successfully upserted 3 chunks to Qdrant for document ID: mcp.md\n",
      "Document processing complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "import pandas as pd\n",
    "import uuid\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    "    TokenTextSplitter,\n",
    "    SpacyTextSplitter,\n",
    "    NLTKTextSplitter,\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    HTMLHeaderTextSplitter,\n",
    "    PythonCodeTextSplitter,\n",
    "    LatexTextSplitter\n",
    ")\n",
    "\n",
    "\n",
    "from services.utils import extract_text\n",
    "\n",
    "\n",
    "# Default configurations\n",
    "DEFAULT_CHUNK_SIZE = 1000\n",
    "DEFAULT_CHUNK_OVERLAP = 200\n",
    "DEFAULT_COLLECTION_NAME = \"mcp\"\n",
    "DEFAULT_QDRANT_HOST = \"localhost\"\n",
    "DEFAULT_QDRANT_PORT = 6333\n",
    "VECTOR_SIZE = 3072\n",
    "\n",
    "\n",
    "class ChunkingMethod:\n",
    "    \"\"\"Available chunking methods with their configurations\"\"\"\n",
    "    RECURSIVE_CHARACTER = \"recursive_character\"\n",
    "    CHARACTER = \"character\"\n",
    "    TOKEN = \"token\"\n",
    "    SPACY = \"spacy\"\n",
    "    NLTK = \"nltk\"\n",
    "    MARKDOWN_HEADER = \"markdown_header\"\n",
    "    HTML_HEADER = \"html_header\"\n",
    "    PYTHON_CODE = \"python_code\"\n",
    "    LATEX = \"latex\"\n",
    "    # Added for consistency, although not fully implemented in your provided code\n",
    "    CUSTOM_TOKEN = \"custom_token\" \n",
    "\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, \n",
    "                 collection_name: str = DEFAULT_COLLECTION_NAME,\n",
    "                 qdrant_host: str = DEFAULT_QDRANT_HOST,\n",
    "                 qdrant_port: int = DEFAULT_QDRANT_PORT,\n",
    "                 embedding_model: AzureOpenAIEmbeddings = None,\n",
    "                 vector_size: int = VECTOR_SIZE):\n",
    "        \"\"\"\n",
    "        Initialize DocumentProcessor with configuration parameters.\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the Qdrant collection\n",
    "            qdrant_host: Qdrant server host\n",
    "            qdrant_port: Qdrant server port\n",
    "            embedding_model: Pre-configured AzureOpenAIEmbeddings instance\n",
    "            vector_size: Expected vector dimension for the collection\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.vector_size = vector_size\n",
    "        self.client = QdrantClient(host=qdrant_host, port=qdrant_port)\n",
    "        \n",
    "        if not self.client.collection_exists(self.collection_name):\n",
    "            print(f\"Collection '{self.collection_name}' does not exist. Creating it.\")\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Collection '{self.collection_name}' already exists.\")\n",
    "\n",
    "        # Use provided embedding model or raise error if not provided\n",
    "        if embedding_model is None:\n",
    "            raise ValueError(\"embedding_model must be provided. Please configure AzureOpenAIEmbeddings externally and pass it to DocumentProcessor.\")\n",
    "        \n",
    "        self.embedding_model = embedding_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text(file_path: str) -> str:\n",
    "        \"\"\"Extract text from file\"\"\"\n",
    "        return extract_text(file_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_text_splitter(\n",
    "        method: str = ChunkingMethod.RECURSIVE_CHARACTER,\n",
    "        chunk_size: int = DEFAULT_CHUNK_SIZE,\n",
    "        chunk_overlap: int = DEFAULT_CHUNK_OVERLAP,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Get the appropriate text splitter based on method\"\"\"\n",
    "        \n",
    "        if method == ChunkingMethod.RECURSIVE_CHARACTER:\n",
    "            return RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=len,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "                **kwargs\n",
    "            )\n",
    "        \n",
    "        elif method == ChunkingMethod.CHARACTER:\n",
    "            return CharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                separator=kwargs.get('separator', '\\n\\n'),\n",
    "                **kwargs\n",
    "            )\n",
    "        \n",
    "        elif method == ChunkingMethod.TOKEN:\n",
    "            return TokenTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                encoding_name=kwargs.get('encoding_name', 'cl100k_base'),\n",
    "                **kwargs\n",
    "            )\n",
    "        \n",
    "        elif method == ChunkingMethod.SPACY:\n",
    "            return SpacyTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                **kwargs\n",
    "            )\n",
    "        \n",
    "        elif method == ChunkingMethod.NLTK:\n",
    "            return NLTKTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                **kwargs\n",
    "            )\n",
    "        \n",
    "        elif method == ChunkingMethod.MARKDOWN_HEADER:\n",
    "            headers_to_split_on = kwargs.get('headers_to_split_on', [\n",
    "                (\"#\", \"Header 1\"),\n",
    "                (\"##\", \"Header 2\"),\n",
    "                (\"###\", \"Header 3\"),\n",
    "            ])\n",
    "            return MarkdownHeaderTextSplitter(\n",
    "                headers_to_split_on=headers_to_split_on,\n",
    "                **kwargs\n",
    "            )\n",
    "        \n",
    "        elif method == ChunkingMethod.HTML_HEADER:\n",
    "            headers_to_split_on = kwargs.get('headers_to_split_on', [\n",
    "                (\"h1\", \"Header 1\"),\n",
    "                (\"h2\", \"Header 2\"),\n",
    "                (\"h3\", \"Header 3\"),\n",
    "            ])\n",
    "            return HTMLHeaderTextSplitter(\n",
    "                headers_to_split_on=headers_to_split_on,\n",
    "                **kwargs\n",
    "            )\n",
    "        \n",
    "        elif method == ChunkingMethod.PYTHON_CODE:\n",
    "            return PythonCodeTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                **kwargs\n",
    "            )\n",
    "        \n",
    "        elif method == ChunkingMethod.LATEX:\n",
    "            return LatexTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                **kwargs\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            # Default to recursive character splitter\n",
    "            return RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                **kwargs\n",
    "            )\n",
    "        \n",
    "    \n",
    "    # Changed to instance method as it relies on self.client and self.embedding_model\n",
    "    def process_and_add_chunks_to_qdrant(self, \n",
    "        text: str, \n",
    "        method: str = ChunkingMethod.RECURSIVE_CHARACTER,\n",
    "        chunk_size: int = DEFAULT_CHUNK_SIZE, \n",
    "        overlap: int = DEFAULT_CHUNK_OVERLAP,\n",
    "        file_type: str = None,\n",
    "        document_name: str = None,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        \"\"\"Split text into chunks using various methods and add them to Qdrant\"\"\"\n",
    "        \n",
    "        # Use custom token method if specified (placeholder for now)\n",
    "        if method == ChunkingMethod.CUSTOM_TOKEN:\n",
    "            # This method needs to be implemented if CUSTOM_TOKEN is to be used\n",
    "            # For now, it will raise an error if called.\n",
    "            raise NotImplementedError(\"Custom token chunking method is not implemented.\")\n",
    "        else:\n",
    "            # Auto-select method based on file type if not specified\n",
    "            if method == \"auto\":\n",
    "                if file_type == \"md\":\n",
    "                    method = ChunkingMethod.MARKDOWN_HEADER\n",
    "                elif file_type == \"py\":\n",
    "                    method = ChunkingMethod.PYTHON_CODE\n",
    "                elif file_type == \"tex\":\n",
    "                    method = ChunkingMethod.LATEX\n",
    "                elif file_type == \"html\":\n",
    "                    method = ChunkingMethod.HTML_HEADER\n",
    "                else:\n",
    "                    method = ChunkingMethod.RECURSIVE_CHARACTER\n",
    "            \n",
    "            # Get the appropriate splitter\n",
    "            splitter = self.get_text_splitter(method, chunk_size, overlap, **kwargs)\n",
    "            \n",
    "            # Handle special cases for header-based splitters\n",
    "            if method in [ChunkingMethod.MARKDOWN_HEADER, ChunkingMethod.HTML_HEADER]:\n",
    "                chunks = splitter.split_text(text)\n",
    "                # Convert Document objects to strings if needed\n",
    "                chunks = [chunk.page_content if hasattr(chunk, 'page_content') else str(chunk) for chunk in chunks]\n",
    "            else:\n",
    "                # Standard text splitting\n",
    "                chunks = splitter.split_text(text)\n",
    "        \n",
    "        # Add each chunk to Qdrant\n",
    "        points_to_upsert = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Embed documents accepts a list, and returns a list of embeddings.\n",
    "            # We want the first (and only) embedding for the current chunk.\n",
    "            try:\n",
    "                embedded_text = self.embedding_model.embed_documents([chunk])[0] \n",
    "            except Exception as e:\n",
    "                print(f\"Error embedding chunk {i}: {e}\")\n",
    "                continue # Skip this chunk if embedding fails\n",
    "\n",
    "            metadata = {\n",
    "                \"chunk_id\": i,\n",
    "                \"document_name\": document_name,\n",
    "                \"text\": chunk, # Store the actual text chunk for retrieval\n",
    "                \"chunk_method\": method,\n",
    "                \"file_type\": file_type\n",
    "            }\n",
    "            points_to_upsert.append(PointStruct(id=str(uuid.uuid4()), vector=embedded_text, payload=metadata))\n",
    "        \n",
    "        if points_to_upsert:\n",
    "            try:\n",
    "                self.client.upsert(\n",
    "                    collection_name=self.collection_name,\n",
    "                    wait=True, # Wait for operation to complete\n",
    "                    points=points_to_upsert\n",
    "                )\n",
    "                print(f\"Successfully upserted {len(points_to_upsert)} chunks to Qdrant for document ID: {document_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error upserting points to Qdrant for document ID {document_name}: {e}\")\n",
    "        else:\n",
    "            print(f\"No chunks to upsert for document ID: {document_name}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage - configuration should be loaded externally\n",
    "    # Load Azure OpenAI configuration from environment variables\n",
    "    azure_embedding_endpoint = os.getenv(\"AZURE_OPENAI_EMBEDDING_ENDPOINT\")\n",
    "    azure_embedding_api_key = os.getenv(\"AZURE_OPENAI_EMBEDDING_API_KEY\")\n",
    "    azure_embedding_model = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\")\n",
    "    azure_embedding_api_version = os.getenv(\"AZURE_OPENAI_EMBEDDING_API_VERSION\")\n",
    "\n",
    "    # Create embedding model\n",
    "    embedding_model = AzureOpenAIEmbeddings(\n",
    "        model=azure_embedding_model,\n",
    "        azure_endpoint=azure_embedding_endpoint,\n",
    "        api_key=azure_embedding_api_key,\n",
    "        openai_api_version=azure_embedding_api_version\n",
    "    )\n",
    "\n",
    "    # Create a dummy mcp.md file for demonstration if it doesn't exist\n",
    "    document_path = \"mcp.md\"\n",
    "    if not os.path.exists(document_path):\n",
    "        print(f\"Creating a dummy file: {document_path}\")\n",
    "        with open(document_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"# Meeting Minutes\\n\\n\")\n",
    "            f.write(\"## June 19, 2025\\n\\n\")\n",
    "            f.write(\"### Attendees\\n\")\n",
    "            f.write(\"- Alice\\n\")\n",
    "            f.write(\"- Bob\\n\")\n",
    "            f.write(\"- Charlie\\n\\n\")\n",
    "            f.write(\"### Discussion Points\\n\")\n",
    "            f.write(\"1.  **Project Alpha**: Reviewed progress. On track for phase 1 completion.\\n\")\n",
    "            f.write(\"2.  **Budget Review**: Discussed Q2 expenditures. Need to optimize cloud spending.\\n\")\n",
    "            f.write(\"3.  **New Initiatives**: Brainstormed ideas for next quarter. Focus on AI integration.\\n\\n\")\n",
    "            f.write(\"### Action Items\\n\")\n",
    "            f.write(\" - Alice: Prepare a detailed report on cloud spending by EOD.\\n\")\n",
    "            f.write(\" - Bob: Research potential AI integration partners.\\n\")\n",
    "            f.write(\" - Charlie: Schedule a follow-up meeting for next week.\\n\")\n",
    "    else:\n",
    "        print(f\"Using existing file: {document_path}\")\n",
    "\n",
    "    processor = DocumentProcessor(\n",
    "        collection_name=\"mcp\",\n",
    "        qdrant_host=\"localhost\",\n",
    "        qdrant_port=6333,\n",
    "        embedding_model=embedding_model,\n",
    "        vector_size=VECTOR_SIZE  # Adjust based on your embedding model\n",
    "    )\n",
    "\n",
    "    document_name = document_path\n",
    "\n",
    "    try:\n",
    "        # Extract text from the markdown file\n",
    "        print(f\"Extracting text from {document_path}...\")\n",
    "        document_text = processor.extract_text(document_path)\n",
    "        print(f\"Text extracted (first 200 chars): {document_text[:200]}...\")\n",
    "        # Process and add chunks to Qdrant\n",
    "        print(f\"Processing and adding chunks to Qdrant using '{ChunkingMethod.MARKDOWN_HEADER}' method...\")\n",
    "        # Use 'auto' or 'markdown_header' explicitly for .md files\n",
    "        processor.process_and_add_chunks_to_qdrant(\n",
    "            text=document_text,\n",
    "            method=\"auto\", # This will correctly identify markdown\n",
    "            file_type=\"md\",\n",
    "            document_name=document_name,\n",
    "        )\n",
    "        print(\"Document processing complete.\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during processing: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException, UploadFile, File, Form, Query\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List, Dict, Any, Literal\n",
    "import os\n",
    "import asyncio\n",
    "import base64\n",
    "import json\n",
    "\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Document Management Routes\n",
    "# ----------------------------\n",
    "@app.post(\"/documents/upload\")\n",
    "async def upload_document(file: UploadFile = File(...), \n",
    "                         metadata: Optional[str] = Form(None)):\n",
    "    \"\"\"Upload a document file\"\"\"\n",
    "    try:\n",
    "        # Read file content\n",
    "        file_content = await file.read()\n",
    "        file_base64 = base64.b64encode(file_content).decode('utf-8')\n",
    "        \n",
    "        # Parse metadata if provided\n",
    "        parsed_metadata = None\n",
    "        if metadata:\n",
    "            try:\n",
    "                parsed_metadata = json.loads(metadata)\n",
    "            except json.JSONDecodeError:\n",
    "                raise HTTPException(status_code=400, detail=\"Invalid metadata JSON format\")\n",
    "        \n",
    "        # Call document service via MCP (matching the function signature from mcp_server_document.py)\n",
    "        result = await mcp_client.call_tool(\"DocumentService\", \"upload_document\", {\n",
    "            \"file_content\": file_base64,\n",
    "            \"filename\": file.filename\n",
    "        })\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# ----------------------------\n",
    "# Health Check\n",
    "# ----------------------------\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"agent_initialized\": agent is not None,\n",
    "        \"mcp_client_initialized\": mcp_client is not None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import base64\n",
    "import aiofiles\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "from fastapi import FastAPI, UploadFile, File, HTTPException, Form\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel\n",
    "import httpx\n",
    "import asyncio\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"Document Upload Service\",\n",
    "    description=\"Service for uploading and processing documents with vector embeddings\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "UPLOAD_DIR = Path(\"./uploads\")\n",
    "UPLOAD_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MCP_SERVER_URL = \"http://localhost:8001\"  # Your MCP server URL\n",
    "MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB limit\n",
    "ALLOWED_EXTENSIONS = {\n",
    "    'pdf', 'docx', 'doc', 'txt', 'md', 'html', 'htm', \n",
    "    'rtf', 'csv', 'xlsx', 'xls', 'pptx', 'ppt'\n",
    "}\n",
    "\n",
    "# Response models\n",
    "class DocumentUploadResponse(BaseModel):\n",
    "    status: str\n",
    "    document_id: Optional[str] = None\n",
    "    filename: str\n",
    "    file_path: Optional[str] = None\n",
    "    collection_id: Optional[str] = None\n",
    "    upload_time: str\n",
    "    processing_status: str\n",
    "    error: Optional[str] = None\n",
    "\n",
    "class DocumentProcessingRequest(BaseModel):\n",
    "    file_content: str\n",
    "    filename: str\n",
    "    metadata: Optional[Dict[str, Any]] = None\n",
    "\n",
    "# Utility functions\n",
    "def get_file_extension(filename: str) -> str:\n",
    "    \"\"\"Extract file extension from filename\"\"\"\n",
    "    return filename.split('.')[-1].lower() if '.' in filename else ''\n",
    "\n",
    "def is_allowed_file(filename: str) -> bool:\n",
    "    \"\"\"Check if file extension is allowed\"\"\"\n",
    "    return get_file_extension(filename) in ALLOWED_EXTENSIONS\n",
    "\n",
    "async def call_mcp_server(file_content: str, filename: str) -> Dict[str, Any]:\n",
    "    \"\"\"Call MCP server to process the document\"\"\"\n",
    "    try:\n",
    "        async with httpx.AsyncClient(timeout=300.0) as client:  # 5 minute timeout\n",
    "            response = await client.post(\n",
    "                f\"{MCP_SERVER_URL}/tools/upload_document\",\n",
    "                json={\n",
    "                    \"file_content\": file_content,\n",
    "                    \"filename\": filename\n",
    "                },\n",
    "                headers={\"Content-Type\": \"application/json\"}\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            else:\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": f\"MCP server error: {response.status_code} - {response.text}\"\n",
    "                }\n",
    "    except httpx.TimeoutException:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": \"MCP server timeout - processing may take longer than expected\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": f\"Failed to connect to MCP server: {str(e)}\"\n",
    "        }\n",
    "\n",
    "# Main upload endpoint\n",
    "@app.post(\"/upload\", response_model=DocumentUploadResponse)\n",
    "async def upload_document(\n",
    "    file: UploadFile = File(...),\n",
    "    metadata: Optional[str] = Form(None)  # JSON string of metadata\n",
    ") -> DocumentUploadResponse:\n",
    "    \"\"\"\n",
    "    Upload a document file, store it locally, and send to MCP server for processing.\n",
    "    \n",
    "    Args:\n",
    "        file: The uploaded file\n",
    "        metadata: Optional JSON string containing additional metadata\n",
    "    \n",
    "    Returns:\n",
    "        DocumentUploadResponse with upload and processing status\n",
    "    \"\"\"\n",
    "    upload_time = datetime.now().isoformat()\n",
    "    \n",
    "    try:\n",
    "        # Validate file\n",
    "        if not file.filename:\n",
    "            raise HTTPException(status_code=400, detail=\"No filename provided\")\n",
    "        \n",
    "        if not is_allowed_file(file.filename):\n",
    "            raise HTTPException(\n",
    "                status_code=400, \n",
    "                detail=f\"File type not allowed. Supported types: {', '.join(ALLOWED_EXTENSIONS)}\"\n",
    "            )\n",
    "        \n",
    "        # Check file size\n",
    "        file_content = await file.read()\n",
    "        if len(file_content) > MAX_FILE_SIZE:\n",
    "            raise HTTPException(\n",
    "                status_code=413, \n",
    "                detail=f\"File too large. Maximum size: {MAX_FILE_SIZE // (1024*1024)}MB\"\n",
    "            )\n",
    "        \n",
    "        # Generate unique document ID and save file\n",
    "        document_id = str(uuid.uuid4())\n",
    "        file_path = UPLOAD_DIR / f\"{document_id}_{file.filename}\"\n",
    "        \n",
    "        # Save file to uploads directory\n",
    "        async with aiofiles.open(file_path, 'wb') as f:\n",
    "            await f.write(file_content)\n",
    "        \n",
    "        # Encode file content to base64 for MCP server\n",
    "        file_content_b64 = base64.b64encode(file_content).decode('utf-8')\n",
    "        \n",
    "        # Process metadata if provided\n",
    "        parsed_metadata = None\n",
    "        if metadata:\n",
    "            try:\n",
    "                import json\n",
    "                parsed_metadata = json.loads(metadata)\n",
    "            except json.JSONDecodeError:\n",
    "                # If metadata is invalid JSON, treat as string\n",
    "                parsed_metadata = {\"note\": metadata}\n",
    "        \n",
    "        # Send to MCP server for processing\n",
    "        mcp_response = await call_mcp_server(\n",
    "            file_content=file_content_b64,\n",
    "            filename=file.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 404, message='Attempt to decode JSON with unexpected mimetype: text/plain; charset=utf-8', url='http://localhost:8001/'\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "import uuid\n",
    "import os\n",
    "\n",
    "async def test_process_document(file_path: str):\n",
    "    \"\"\"Simple test function to process a document\"\"\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    # Prepare request\n",
    "    filename = os.path.basename(file_path)\n",
    "    document_id = str(uuid.uuid4())\n",
    "    \n",
    "    payload = {\n",
    "        \"method\": \"tools/call\",\n",
    "        \"params\": {\n",
    "            \"name\": \"process_document\",\n",
    "            \"arguments\": {\n",
    "                \"file_path\": file_path,\n",
    "                \"filename\": filename,\n",
    "                \"document_id\": document_id\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Call the service\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(\n",
    "                \"http://localhost:8001/\",\n",
    "                json=payload,\n",
    "                headers={\"Content-Type\": \"application/json\"}\n",
    "            ) as response:\n",
    "                result = await response.json()\n",
    "                print(f\"Status: {response.status}\")\n",
    "                print(f\"Result: {json.dumps(result, indent=2)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "# Test with your file path\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"data\\mcp.md\"\n",
    "    try:\n",
    "        # Try to run in existing event loop (Jupyter)\n",
    "        await test_process_document(file_path)\n",
    "    except:\n",
    "        # Fallback for normal Python script\n",
    "        asyncio.run(test_process_document(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m             \u001b[38;5;28mprint\u001b[39m(result)  \u001b[38;5;66;03m# 12\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01masyncio\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\runners.py:186\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    187\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "from mcp.client.session import ClientSession\n",
    "\n",
    "async def main():\n",
    "    async with streamablehttp_client(\"http://localhost:8009/mcp\") as (read, write, _):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "\n",
    "            result = await session.call_tool(\"get_weather\", {\"location\": \"Warsaw\"})\n",
    "            print(result)  # 12\n",
    "\n",
    "import asyncio\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
